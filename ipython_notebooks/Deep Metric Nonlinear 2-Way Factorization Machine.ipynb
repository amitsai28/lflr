{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:\n",
    "=====\n",
    "\n",
    "basically a factorization machine with cross entropy loss where interaction effects come from deep nonlinear relu-activated embeddings and with an additional \"metric\" kernal matrix.\n",
    "\n",
    "todo: dropout. currently no regularization on the interaction layers in the cost function. can handle with FTRL optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import this stuff\n",
    "import time\n",
    "import sys\n",
    "from pylab import *\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_embeddings(x, rank, num_features, depth=1, seed=12345):\n",
    "    \"\"\"\n",
    "      assumes that all hidden layers are width `rank`\n",
    "    \"\"\"\n",
    "    assert depth > 0\n",
    "    V = tf.Variable(tf.truncated_normal([rank, num_features], stddev=0.2, mean=0, seed=seed), name=\"v_1\")\n",
    "    b = tf.Variable(tf.truncated_normal([rank, 1], stddev=0.2, mean=0, seed=seed), name=\"b_1\")\n",
    "    Vx = tf.nn.relu(tf.matmul(V, x) + b)\n",
    "    for i in range(depth - 1):\n",
    "        V = tf.Variable(tf.truncated_normal([rank, rank], stddev=0.2, mean=0, seed=seed), name=\"v_%s\" % i)\n",
    "        b = tf.Variable(tf.truncated_normal([rank, 1], stddev=0.2, mean=0, seed=seed), name=\"b_%s\" % i)\n",
    "        Vx = tf.nn.relu(tf.matmul(V, Vx) + b)\n",
    "\n",
    "    return Vx\n",
    "\n",
    "def factorize(observed_features,\n",
    "              labels,\n",
    "              observed_features_validation,\n",
    "              labels_validation,\n",
    "              rank,\n",
    "              max_iter=100,\n",
    "              verbose=False,\n",
    "              lambda_v=0,\n",
    "              lambda_k=0,\n",
    "              lambda_w=0,\n",
    "              lambda_constants=0,\n",
    "              epsilon=0.001,\n",
    "              optimizer=tf.train.AdamOptimizer(),\n",
    "              depth=3,\n",
    "              seed=12345):\n",
    "\n",
    "    # Extract info about shapes etc from the training data\n",
    "    num_items = observed_features.shape[0]\n",
    "    num_features = observed_features.shape[1]\n",
    "    K = tf.Variable(tf.truncated_normal([rank, rank], stddev=0.2, mean=0, seed=seed), name=\"metric_matrix\")\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([1, num_features], stddev=0.2, mean=0, seed=seed), name=\"hyperplane\")\n",
    "    b = tf.Variable(tf.truncated_normal([1, 1], stddev=0.2, mean=0, seed=seed), name=\"b_one\")\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, num_features])\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    \n",
    "    norm_x = tf.nn.l2_normalize(x, dim=0)\n",
    "    \n",
    "    Vx = make_embeddings(tf.transpose(norm_x), rank, num_features, depth=depth, seed=seed)\n",
    "    right_kern = tf.matmul(K, Vx)\n",
    "    \n",
    "    full_kern = tf.matmul(tf.transpose(Vx), right_kern)\n",
    "    linear = tf.matmul(w, tf.transpose(norm_x))\n",
    "\n",
    "    pred = tf.reduce_sum(tf.sigmoid(linear + full_kern + b))\n",
    "    \n",
    "    # todo: dropout. currently no regularization on the interaction layers in the cost functino\n",
    "    # can handle with FTRL optimization\n",
    "    cost = tf.reduce_mean(-y*tf.log(pred + 0.0000000001) - (1-y)*tf.log((1-pred + 0.0000000001)) + \n",
    "            lambda_k*tf.nn.l2_loss(K) +\n",
    "            lambda_w*tf.nn.l2_loss(w) +\n",
    "            lambda_constants*tf.nn.l2_loss(b))\n",
    "    optimize = optimizer.minimize(cost)\n",
    "    norm = tf.reduce_mean(tf.nn.l2_loss(w))\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        last_cost = 1000000\n",
    "        for iter in range(0, max_iter):\n",
    "            avg_cost = 0\n",
    "            \n",
    "            for i in range(num_items):\n",
    "                _, c, n = sess.run([optimize, cost, norm],\n",
    "                              feed_dict={x:observed_features[i].reshape(1, num_features), y:labels[i]})\n",
    "                avg_cost += c / num_items\n",
    "            if verbose:\n",
    "                print(\"epoch: %s, cost: %s\" % (iter+1, avg_cost))\n",
    "\n",
    "            # check for convergence\n",
    "            if abs(avg_cost-last_cost)/avg_cost < epsilon:\n",
    "                break\n",
    "                \n",
    "            last_cost = avg_cost\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"optimization finished\")\n",
    "        predictions = []\n",
    "        total_costs = 0\n",
    "        for i in range(observed_features_validation.shape[0]):\n",
    "            p, c = sess.run([pred, cost], feed_dict={x:observed_features_validation[i].reshape(1, num_features), y:labels_validation[i]})\n",
    "            predictions.append(p)\n",
    "            total_costs += c\n",
    "        return predictions, total_costs/observed_features_validation.shape[0], sess.run([norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this data for now\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "ng = datasets.fetch_20newsgroups (categories=categories, shuffle=True)\n",
    "labels = [1 if y == 2 else 0 for y in ng.target.reshape(-1,1)]\n",
    "\n",
    "tfidf = TfidfVectorizer(decode_error=False, min_df=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ng.data, labels, test_size=.3)\n",
    "X_train = tfidf.fit_transform(X_train).todense()\n",
    "X_test = tfidf.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.33360196478\n",
      "epoch: 2, cost: 0.0401097349682\n",
      "epoch: 3, cost: 0.00931769804643\n",
      "epoch: 4, cost: 0.00247838839455\n",
      "epoch: 5, cost: 0.000511754474805\n",
      "epoch: 6, cost: 0.000126325859087\n",
      "epoch: 7, cost: 4.04279771517e-05\n",
      "epoch: 8, cost: 1.0098687531e-05\n",
      "epoch: 9, cost: 2.85424085244e-06\n",
      "epoch: 10, cost: 8.69213185562e-07\n",
      "epoch: 11, cost: 2.59260408291e-07\n",
      "epoch: 12, cost: 8.90110950503e-08\n",
      "epoch: 13, cost: 3.41245779331e-08\n",
      "epoch: 14, cost: 1.30231913253e-08\n",
      "epoch: 15, cost: 5.20927401194e-09\n",
      "epoch: 16, cost: 1.92516616005e-09\n",
      "epoch: 17, cost: 6.79470328429e-10\n",
      "epoch: 18, cost: 2.64238450556e-10\n",
      "epoch: 19, cost: 7.54967001588e-11\n",
      "epoch: 20, cost: 7.54967001588e-11\n",
      "optimization finished\n",
      "rank: 10, cost: 0.533316462163, overall AUC: 0.96627069576, norm: [111.30756]\n"
     ]
    }
   ],
   "source": [
    "r = 10\n",
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, r, verbose=True, lambda_v=0.1, max_iter=300)\n",
    "print(\"rank: %s, cost: %s, overall AUC: %s, norm: %s\") % (r, test_costs, roc_auc_score(y_test, predictions, average=\"weighted\"), norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.163082238403\n",
      "epoch: 2, cost: 0.0300150069038\n",
      "epoch: 3, cost: 0.020797190683\n",
      "epoch: 4, cost: 0.0161998612776\n",
      "epoch: 5, cost: 0.0133454948845\n",
      "epoch: 6, cost: 0.0113824729583\n",
      "epoch: 7, cost: 0.00994217756297\n",
      "epoch: 8, cost: 0.00883659067324\n",
      "epoch: 9, cost: 0.00795914143351\n",
      "epoch: 10, cost: 0.0072446552217\n",
      "epoch: 11, cost: 0.00665089781192\n",
      "epoch: 12, cost: 0.00614922306421\n",
      "epoch: 13, cost: 0.00571947609275\n",
      "epoch: 14, cost: 0.00534703658665\n",
      "epoch: 15, cost: 0.00502103509521\n",
      "epoch: 16, cost: 0.00473320190374\n",
      "epoch: 17, cost: 0.00447714238629\n",
      "epoch: 18, cost: 0.00424782052873\n",
      "epoch: 19, cost: 0.00404122097294\n",
      "epoch: 20, cost: 0.00385409398374\n",
      "epoch: 21, cost: 0.00368379288721\n",
      "epoch: 22, cost: 0.00352812143286\n",
      "epoch: 23, cost: 0.00338526291288\n",
      "epoch: 24, cost: 0.00325368161432\n",
      "epoch: 25, cost: 0.00313208631847\n",
      "epoch: 26, cost: 0.00301937139572\n",
      "epoch: 27, cost: 0.00291459122074\n",
      "epoch: 28, cost: 0.00281693069027\n",
      "epoch: 29, cost: 0.00272568343692\n",
      "epoch: 30, cost: 0.00264023278333\n",
      "epoch: 31, cost: 0.0025600409671\n",
      "epoch: 32, cost: 0.00248463289607\n",
      "epoch: 33, cost: 0.00241358713369\n",
      "epoch: 34, cost: 0.00234653804256\n",
      "epoch: 35, cost: 0.00228315363006\n",
      "epoch: 36, cost: 0.00222313984101\n",
      "epoch: 37, cost: 0.00216623205883\n",
      "epoch: 38, cost: 0.0021121949152\n",
      "epoch: 39, cost: 0.00206081321367\n",
      "epoch: 40, cost: 0.0020118967125\n",
      "epoch: 41, cost: 0.00196527369045\n",
      "epoch: 42, cost: 0.00192077992923\n",
      "epoch: 43, cost: 0.00187827682487\n",
      "epoch: 44, cost: 0.00183762950408\n",
      "epoch: 45, cost: 0.00179872102377\n",
      "epoch: 46, cost: 0.00176144118022\n",
      "epoch: 47, cost: 0.00172568677439\n",
      "epoch: 48, cost: 0.00169136837617\n",
      "epoch: 49, cost: 0.00165840040811\n",
      "epoch: 50, cost: 0.00162670228833\n",
      "epoch: 51, cost: 0.0015962051135\n",
      "epoch: 52, cost: 0.0015668385645\n",
      "epoch: 53, cost: 0.00153854203156\n",
      "epoch: 54, cost: 0.00151125879522\n",
      "epoch: 55, cost: 0.00148493267265\n",
      "epoch: 56, cost: 0.00145951561066\n",
      "epoch: 57, cost: 0.00143496012367\n",
      "epoch: 58, cost: 0.00141122377182\n",
      "epoch: 59, cost: 0.00138826623933\n",
      "epoch: 60, cost: 0.00136605083435\n",
      "epoch: 61, cost: 0.00134453893431\n",
      "epoch: 62, cost: 0.00132369854504\n",
      "epoch: 63, cost: 0.00130350006921\n",
      "epoch: 64, cost: 0.00128391297868\n",
      "epoch: 65, cost: 0.00126491000866\n",
      "epoch: 66, cost: 0.00124646427853\n",
      "epoch: 67, cost: 0.00122855340616\n",
      "epoch: 68, cost: 0.00121115542034\n",
      "epoch: 69, cost: 0.00119424501925\n",
      "epoch: 70, cost: 0.00117780405182\n",
      "epoch: 71, cost: 0.00116181278799\n",
      "epoch: 72, cost: 0.00114625395095\n",
      "epoch: 73, cost: 0.00113110876741\n",
      "epoch: 74, cost: 0.00111636044691\n",
      "epoch: 75, cost: 0.00110199567985\n",
      "epoch: 76, cost: 0.00108799735545\n",
      "epoch: 77, cost: 0.00107435344647\n",
      "epoch: 78, cost: 0.00106104985876\n",
      "epoch: 79, cost: 0.00104807387354\n",
      "epoch: 80, cost: 0.00103541220175\n",
      "epoch: 81, cost: 0.00102305557215\n",
      "epoch: 82, cost: 0.00101099351647\n",
      "epoch: 83, cost: 0.000999213428788\n",
      "epoch: 84, cost: 0.00098770654027\n",
      "epoch: 85, cost: 0.000976464046743\n",
      "epoch: 86, cost: 0.000965476684345\n",
      "epoch: 87, cost: 0.000954734515838\n",
      "epoch: 88, cost: 0.00094422994616\n",
      "epoch: 89, cost: 0.00093395654283\n",
      "epoch: 90, cost: 0.000923905004273\n",
      "epoch: 91, cost: 0.000914070752525\n",
      "epoch: 92, cost: 0.000904443083365\n",
      "epoch: 93, cost: 0.000895017265347\n",
      "epoch: 94, cost: 0.000885788802021\n",
      "epoch: 95, cost: 0.000876747394487\n",
      "epoch: 96, cost: 0.000867892512208\n",
      "epoch: 97, cost: 0.000859215859906\n",
      "epoch: 98, cost: 0.000850711152251\n",
      "epoch: 99, cost: 0.000842374326623\n",
      "epoch: 100, cost: 0.000834199634697\n",
      "epoch: 101, cost: 0.000826184351744\n",
      "epoch: 102, cost: 0.000818321981594\n",
      "epoch: 103, cost: 0.000810609613494\n",
      "epoch: 104, cost: 0.000803039844506\n",
      "epoch: 105, cost: 0.000795612083869\n",
      "epoch: 106, cost: 0.000788323583679\n",
      "epoch: 107, cost: 0.000781165413064\n",
      "epoch: 108, cost: 0.000774138749265\n",
      "epoch: 109, cost: 0.000767237317759\n",
      "epoch: 110, cost: 0.00076045798542\n",
      "epoch: 111, cost: 0.000753800192806\n",
      "epoch: 112, cost: 0.000747256707347\n",
      "epoch: 113, cost: 0.000740827103845\n",
      "epoch: 114, cost: 0.000734507372173\n",
      "epoch: 115, cost: 0.000728297206285\n",
      "epoch: 116, cost: 0.000722190742396\n",
      "epoch: 117, cost: 0.000716185876739\n",
      "epoch: 118, cost: 0.00071028108737\n",
      "epoch: 119, cost: 0.000704474867419\n",
      "epoch: 120, cost: 0.000698761066613\n",
      "epoch: 121, cost: 0.000693140592124\n",
      "epoch: 122, cost: 0.000687610361405\n",
      "epoch: 123, cost: 0.000682169312587\n",
      "epoch: 124, cost: 0.000676813495838\n",
      "epoch: 125, cost: 0.000671539968767\n",
      "epoch: 126, cost: 0.000666349579729\n",
      "epoch: 127, cost: 0.000661239653993\n",
      "epoch: 128, cost: 0.000656208510949\n",
      "epoch: 129, cost: 0.000651253169245\n",
      "epoch: 130, cost: 0.000646373319079\n",
      "epoch: 131, cost: 0.000641564389602\n",
      "epoch: 132, cost: 0.00063682954406\n",
      "epoch: 133, cost: 0.000632163048682\n",
      "epoch: 134, cost: 0.000627564625424\n",
      "epoch: 135, cost: 0.000623033951483\n",
      "epoch: 136, cost: 0.000618568174484\n",
      "epoch: 137, cost: 0.000614166339053\n",
      "epoch: 138, cost: 0.00060982569467\n",
      "epoch: 139, cost: 0.000605547636014\n",
      "epoch: 140, cost: 0.000601329729539\n",
      "epoch: 141, cost: 0.00059717006212\n",
      "epoch: 142, cost: 0.000593068587361\n",
      "epoch: 143, cost: 0.000589022944155\n",
      "epoch: 144, cost: 0.000585031653287\n",
      "epoch: 145, cost: 0.000581095645022\n",
      "epoch: 146, cost: 0.000577211493793\n",
      "epoch: 147, cost: 0.000573380256266\n",
      "epoch: 148, cost: 0.000569598975356\n",
      "epoch: 149, cost: 0.000565868390315\n",
      "epoch: 150, cost: 0.000562185659144\n",
      "epoch: 151, cost: 0.000558551447761\n",
      "epoch: 152, cost: 0.000554964048792\n",
      "epoch: 153, cost: 0.000551422471366\n",
      "epoch: 154, cost: 0.000547925420353\n",
      "epoch: 155, cost: 0.000544472202198\n",
      "epoch: 156, cost: 0.000541065306433\n",
      "epoch: 157, cost: 0.000537699209707\n",
      "epoch: 158, cost: 0.000534376011562\n",
      "epoch: 159, cost: 0.000531092232804\n",
      "epoch: 160, cost: 0.000527849492985\n",
      "epoch: 161, cost: 0.000524646643387\n",
      "epoch: 162, cost: 0.000521482360436\n",
      "epoch: 163, cost: 0.000518356335316\n",
      "epoch: 164, cost: 0.000515267381541\n",
      "epoch: 165, cost: 0.000512216068207\n",
      "epoch: 166, cost: 0.000509200194867\n",
      "epoch: 167, cost: 0.000506220772813\n",
      "epoch: 168, cost: 0.000503275266205\n",
      "epoch: 169, cost: 0.000500363479866\n",
      "epoch: 170, cost: 0.000497486012052\n",
      "epoch: 171, cost: 0.000494642703663\n",
      "epoch: 172, cost: 0.000491830153402\n",
      "epoch: 173, cost: 0.000489050959674\n",
      "epoch: 174, cost: 0.000486301458251\n",
      "epoch: 175, cost: 0.00048358409189\n",
      "epoch: 176, cost: 0.000480897311151\n",
      "epoch: 177, cost: 0.000478239675095\n",
      "epoch: 178, cost: 0.000475610799167\n",
      "epoch: 179, cost: 0.000473012642868\n",
      "epoch: 180, cost: 0.000470441581941\n",
      "epoch: 181, cost: 0.000467898967316\n",
      "epoch: 182, cost: 0.000465384485932\n",
      "epoch: 183, cost: 0.000462896142716\n",
      "epoch: 184, cost: 0.000460432490068\n",
      "epoch: 185, cost: 0.000457997458348\n",
      "epoch: 186, cost: 0.00045558813564\n",
      "epoch: 187, cost: 0.000453203607597\n",
      "epoch: 188, cost: 0.000450844512074\n",
      "epoch: 189, cost: 0.000448508167148\n",
      "epoch: 190, cost: 0.00044619796646\n",
      "epoch: 191, cost: 0.000443911303035\n",
      "epoch: 192, cost: 0.000441647153375\n",
      "epoch: 193, cost: 0.000439407363428\n",
      "epoch: 194, cost: 0.000437190495818\n",
      "epoch: 195, cost: 0.000434994203023\n",
      "epoch: 196, cost: 0.000432821772648\n",
      "epoch: 197, cost: 0.000430669078252\n",
      "epoch: 198, cost: 0.000428539451111\n",
      "epoch: 199, cost: 0.000426431142319\n",
      "epoch: 200, cost: 0.000424342529129\n",
      "epoch: 201, cost: 0.000422273265828\n",
      "epoch: 202, cost: 0.00042022592525\n",
      "epoch: 203, cost: 0.000418197925614\n",
      "epoch: 204, cost: 0.000416189726618\n",
      "epoch: 205, cost: 0.000414201097069\n",
      "epoch: 206, cost: 0.000412230069666\n",
      "epoch: 207, cost: 0.000410279736193\n",
      "epoch: 208, cost: 0.000408346361151\n",
      "epoch: 209, cost: 0.000406432282825\n",
      "epoch: 210, cost: 0.000404535079233\n",
      "epoch: 211, cost: 0.000402657017453\n",
      "epoch: 212, cost: 0.000400795336796\n",
      "epoch: 213, cost: 0.000398950409276\n",
      "epoch: 214, cost: 0.00039712375419\n",
      "epoch: 215, cost: 0.000395311733569\n",
      "epoch: 216, cost: 0.000393518881988\n",
      "epoch: 217, cost: 0.000391741003855\n",
      "epoch: 218, cost: 0.000389979268581\n",
      "epoch: 219, cost: 0.000388232053501\n",
      "epoch: 220, cost: 0.000386502903647\n",
      "epoch: 221, cost: 0.000384788156544\n",
      "epoch: 222, cost: 0.000383089735988\n",
      "epoch: 223, cost: 0.000381404537194\n",
      "epoch: 224, cost: 0.000379733778262\n",
      "epoch: 225, cost: 0.000378079641327\n",
      "epoch: 226, cost: 0.000376438652032\n",
      "epoch: 227, cost: 0.000374811716714\n",
      "epoch: 228, cost: 0.000373199890131\n",
      "epoch: 229, cost: 0.000371600069648\n",
      "epoch: 230, cost: 0.000370016040114\n",
      "epoch: 231, cost: 0.000368444504641\n",
      "epoch: 232, cost: 0.000366886675485\n",
      "epoch: 233, cost: 0.000365341910708\n",
      "epoch: 234, cost: 0.000363809339791\n",
      "epoch: 235, cost: 0.00036229096126\n",
      "epoch: 236, cost: 0.000360783411433\n",
      "epoch: 237, cost: 0.000359291563386\n",
      "epoch: 238, cost: 0.00035780899662\n",
      "epoch: 239, cost: 0.00035633910841\n",
      "epoch: 240, cost: 0.000354883331296\n",
      "epoch: 241, cost: 0.00035343804003\n",
      "epoch: 242, cost: 0.000352004025875\n",
      "epoch: 243, cost: 0.000350584122914\n",
      "epoch: 244, cost: 0.00034917164039\n",
      "epoch: 245, cost: 0.000347772776778\n",
      "epoch: 246, cost: 0.000346385786818\n",
      "epoch: 247, cost: 0.000345008863275\n",
      "epoch: 248, cost: 0.000343643439666\n",
      "epoch: 249, cost: 0.000342288531128\n",
      "epoch: 250, cost: 0.000340943152983\n",
      "epoch: 251, cost: 0.000339610443503\n",
      "epoch: 252, cost: 0.000338287527454\n",
      "epoch: 253, cost: 0.000336974332074\n",
      "epoch: 254, cost: 0.000335672102007\n",
      "epoch: 255, cost: 0.000334379170736\n",
      "epoch: 256, cost: 0.000333096598665\n",
      "epoch: 257, cost: 0.000331823518431\n",
      "epoch: 258, cost: 0.000330559886139\n",
      "epoch: 259, cost: 0.000329305894241\n",
      "epoch: 260, cost: 0.000328061690319\n",
      "epoch: 261, cost: 0.000326828371439\n",
      "epoch: 262, cost: 0.000325602877582\n",
      "epoch: 263, cost: 0.00032438660205\n",
      "epoch: 264, cost: 0.000323179358033\n",
      "epoch: 265, cost: 0.000321981938896\n",
      "epoch: 266, cost: 0.000320792905987\n",
      "epoch: 267, cost: 0.000319611846898\n",
      "epoch: 268, cost: 0.000318440307001\n",
      "epoch: 269, cost: 0.00031727745438\n",
      "epoch: 270, cost: 0.000316124158699\n",
      "epoch: 271, cost: 0.000314976148192\n",
      "epoch: 272, cost: 0.000313839092276\n",
      "epoch: 273, cost: 0.000312709738625\n",
      "epoch: 274, cost: 0.00031158778878\n",
      "epoch: 275, cost: 0.000310473766442\n",
      "epoch: 276, cost: 0.000309368694238\n",
      "epoch: 277, cost: 0.000308270605418\n",
      "epoch: 278, cost: 0.000307181162345\n",
      "epoch: 279, cost: 0.0003061001399\n",
      "epoch: 280, cost: 0.000305024208704\n",
      "epoch: 281, cost: 0.000303957413314\n",
      "epoch: 282, cost: 0.000302899486127\n",
      "epoch: 283, cost: 0.000301846462616\n",
      "epoch: 284, cost: 0.000300800839544\n",
      "epoch: 285, cost: 0.000299763213989\n",
      "epoch: 286, cost: 0.000298733513198\n",
      "epoch: 287, cost: 0.00029771033741\n",
      "epoch: 288, cost: 0.000296694292624\n",
      "epoch: 289, cost: 0.000295684694961\n",
      "epoch: 290, cost: 0.000294682567523\n",
      "epoch: 291, cost: 0.000293686855096\n",
      "epoch: 292, cost: 0.000292697398888\n",
      "epoch: 293, cost: 0.000291714881315\n",
      "epoch: 294, cost: 0.00029073858711\n",
      "epoch: 295, cost: 0.000289770250374\n",
      "epoch: 296, cost: 0.000288807945756\n",
      "epoch: 297, cost: 0.000287851784537\n",
      "epoch: 298, cost: 0.000286900786\n",
      "epoch: 299, cost: 0.000285956197444\n",
      "epoch: 300, cost: 0.000285020394495\n",
      "optimization finished\n",
      "rank: 10, cost: 0.0874918085269, overall AUC: 0.991948990334, norm: [280.67853]\n"
     ]
    }
   ],
   "source": [
    "# with some regularization via the optimizer\n",
    "r = 10\n",
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, r, verbose=True, max_iter=300, optimizer=tf.train.FtrlOptimizer(0.1, l2_regularization_strength=0.1))\n",
    "print(\"rank: %s, cost: %s, overall AUC: %s, norm: %s\") % (r, test_costs, roc_auc_score(y_test, predictions, average=\"weighted\"), norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
