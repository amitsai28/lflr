{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:\n",
    "=====\n",
    "\n",
    "takes the latent factor logistic a bit farther in the direction of deep learning, adding biases and nonlinearites at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import this stuff\n",
    "import time\n",
    "import sys\n",
    "from pylab import *\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def factorize(observed_features,\n",
    "              labels,\n",
    "              observed_features_validation,\n",
    "              labels_validation,\n",
    "              rank,\n",
    "              max_iter=100,\n",
    "              batch_size = 100,\n",
    "              verbose=False,\n",
    "              lambda_v=0,\n",
    "              lambda_u=0,\n",
    "              lambda_constants=0,\n",
    "              epsilon=0.001,\n",
    "              optimizer=tf.train.AdamOptimizer(),\n",
    "              seed=12345):\n",
    "\n",
    "    # Extract info about shapes etc from the training data\n",
    "    num_items = observed_features.shape[0]\n",
    "    num_features = observed_features.shape[1]\n",
    "    num_classes = labels.shape[1]\n",
    " \n",
    "    U = tf.Variable(tf.truncated_normal([rank, num_features], stddev=0.2, mean=0, seed=seed), name=\"item_explainers\")\n",
    "    v_prime = tf.Variable(tf.truncated_normal([num_classes, rank], stddev=0.2, mean=0, seed=seed), name=\"hyperplane\")\n",
    "    b_one = tf.Variable(tf.truncated_normal([rank, 1], stddev=0.2, mean=0, seed=seed), name=\"b_one\")\n",
    "    b_two = tf.Variable(tf.truncated_normal([1, num_classes], stddev=0.2, mean=0, seed=seed), name=\"b_two\")\n",
    "   \n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, num_features])\n",
    "    y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "                           \n",
    "    pred = tf.nn.softmax(b_two + tf.transpose(tf.matmul(v_prime, tf.nn.tanh(tf.matmul(U, tf.transpose(tf.nn.l2_normalize(x, dim=0)))) + b_one)))\n",
    "    \n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred + 0.0000000001), reduction_indices=1) + # this was causing nans if pred == 0\n",
    "                          lambda_v*tf.nn.l2_loss(v_prime)  + # regularization for v\n",
    "                          lambda_u*tf.nn.l2_loss(U) + # regularization for U\n",
    "                          lambda_constants*(tf.nn.l2_loss(b_one) + tf.nn.l2_loss(b_two)))\n",
    "    norm = tf.nn.l2_loss(v_prime)\n",
    "    optimize = optimizer.minimize(cost)\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        last_cost = 1000000\n",
    "        for iter in range(0, max_iter):\n",
    "            avg_cost = 0\n",
    "            batches = int(np.ceil(num_items/batch_size))\n",
    "            xs = np.array_split(observed_features, batches)\n",
    "            ys = np.array_split(labels, batches)\n",
    "            \n",
    "            for i in range(batches):\n",
    "                _, c, n = sess.run([optimize, cost, norm],\n",
    "                                   feed_dict={x:xs[i], y:ys[i]})\n",
    "                avg_cost += c / xs[i].shape[0]\n",
    "            if verbose:\n",
    "                print(\"epoch: %s, cost: %s, norm: %s\" % (iter+1, avg_cost, n))\n",
    "\n",
    "            # check for convergence\n",
    "            if abs(avg_cost-last_cost)/avg_cost < epsilon:\n",
    "                break\n",
    "                \n",
    "            last_cost = avg_cost\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"optimization finished\")\n",
    "        # test prediction\n",
    "        predictions, test_costs, norm = sess.run([pred, cost, norm], feed_dict={x:observed_features_validation, y:labels_validation})\n",
    "        return predictions, test_costs, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this data for now\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "ng = datasets.fetch_20newsgroups (categories=categories, shuffle=True)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels = encoder.fit_transform(ng.target.reshape(-1,1))\n",
    "\n",
    "tfidf = TfidfVectorizer(decode_error=False, min_df=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ng.data, labels, test_size=.3)\n",
    "X_train = tfidf.fit_transform(X_train).todense()\n",
    "X_test = tfidf.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 0.251515544714, norm: 0.571562\n",
      "epoch: 2, cost: 0.224982089099, norm: 0.50757\n",
      "epoch: 3, cost: 0.206132943847, norm: 0.459181\n",
      "epoch: 4, cost: 0.191209653403, norm: 0.426653\n",
      "epoch: 5, cost: 0.178604317076, norm: 0.408867\n",
      "epoch: 6, cost: 0.167633029543, norm: 0.403772\n",
      "epoch: 7, cost: 0.158005029011, norm: 0.40868\n",
      "epoch: 8, cost: 0.149565987274, norm: 0.42065\n",
      "epoch: 9, cost: 0.142213617952, norm: 0.436864\n",
      "epoch: 10, cost: 0.13585215016, norm: 0.454891\n",
      "epoch: 11, cost: 0.130408114779, norm: 0.472859\n",
      "epoch: 12, cost: 0.125762103392, norm: 0.489517\n",
      "epoch: 13, cost: 0.121801529044, norm: 0.504194\n",
      "epoch: 14, cost: 0.118422362473, norm: 0.516653\n",
      "epoch: 15, cost: 0.11552977887, norm: 0.526939\n",
      "epoch: 16, cost: 0.113039929489, norm: 0.53525\n",
      "epoch: 17, cost: 0.110885585399, norm: 0.541849\n",
      "epoch: 18, cost: 0.109014151419, norm: 0.547007\n",
      "epoch: 19, cost: 0.107373186718, norm: 0.550973\n",
      "epoch: 20, cost: 0.105921770219, norm: 0.553978\n",
      "epoch: 21, cost: 0.104630345037, norm: 0.55623\n",
      "epoch: 22, cost: 0.103466617188, norm: 0.557919\n",
      "epoch: 23, cost: 0.10240631291, norm: 0.559208\n",
      "epoch: 24, cost: 0.101454211877, norm: 0.560203\n",
      "epoch: 25, cost: 0.100592115883, norm: 0.560969\n",
      "epoch: 26, cost: 0.0998141645796, norm: 0.561557\n",
      "epoch: 27, cost: 0.0991139697525, norm: 0.562001\n",
      "epoch: 28, cost: 0.0984826889642, norm: 0.562324\n",
      "epoch: 29, cost: 0.0979121734136, norm: 0.562547\n",
      "epoch: 30, cost: 0.0973950559827, norm: 0.56269\n",
      "epoch: 31, cost: 0.0969240435211, norm: 0.562767\n",
      "epoch: 32, cost: 0.0964957501582, norm: 0.562792\n",
      "epoch: 33, cost: 0.0961071246986, norm: 0.56278\n",
      "epoch: 34, cost: 0.0957516075091, norm: 0.562741\n",
      "epoch: 35, cost: 0.0954221701954, norm: 0.562687\n",
      "epoch: 36, cost: 0.0951090759505, norm: 0.562629\n",
      "epoch: 37, cost: 0.094821864977, norm: 0.562564\n",
      "epoch: 38, cost: 0.0945593400893, norm: 0.562473\n",
      "epoch: 39, cost: 0.0943149218394, norm: 0.562354\n",
      "epoch: 40, cost: 0.0940867402804, norm: 0.562217\n",
      "epoch: 41, cost: 0.09387246471, norm: 0.562073\n",
      "epoch: 42, cost: 0.0936698441289, norm: 0.56193\n",
      "epoch: 43, cost: 0.0934771672681, norm: 0.561787\n",
      "epoch: 44, cost: 0.0932937294111, norm: 0.561645\n",
      "epoch: 45, cost: 0.0931205046413, norm: 0.561502\n",
      "epoch: 46, cost: 0.0929571115692, norm: 0.561355\n",
      "epoch: 47, cost: 0.0928016516023, norm: 0.561206\n",
      "epoch: 48, cost: 0.09265277262, norm: 0.561056\n",
      "epoch: 49, cost: 0.0925098853398, norm: 0.560906\n",
      "epoch: 50, cost: 0.0923729838601, norm: 0.560757\n",
      "epoch: 51, cost: 0.092242471286, norm: 0.560613\n",
      "epoch: 52, cost: 0.0921186479597, norm: 0.560475\n",
      "epoch: 53, cost: 0.0920015405184, norm: 0.560341\n",
      "epoch: 54, cost: 0.0918907228506, norm: 0.560208\n",
      "epoch: 55, cost: 0.091785377987, norm: 0.560076\n",
      "epoch: 56, cost: 0.0916844207536, norm: 0.559946\n",
      "epoch: 57, cost: 0.0915866082944, norm: 0.55982\n",
      "epoch: 58, cost: 0.0914904944016, norm: 0.559697\n",
      "epoch: 59, cost: 0.0913947198001, norm: 0.559577\n",
      "epoch: 60, cost: 0.0913044848586, norm: 0.559452\n",
      "optimization finished\n",
      "rank: 10, cost: 0.916932, norm: 0.559472\n",
      "class 0 AUC: 0.980476056579\n",
      "class 1 AUC: 0.997053349876\n",
      "class 2 AUC: 0.991030789826\n",
      "class 3 AUC: 0.994049999394\n",
      "overall AUC: 0.990911419812\n"
     ]
    }
   ],
   "source": [
    "r = 10\n",
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, r, verbose=True, lambda_v=0.5, max_iter=300)\n",
    "print(\"rank: %s, cost: %s, norm: %s\") % (r, test_costs, norm)\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(\"class %s AUC: %s\") % (i, roc_auc_score(y_test[:,i], predictions[:,i]))\n",
    "print(\"overall AUC: %s\") % roc_auc_score(y_test, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
