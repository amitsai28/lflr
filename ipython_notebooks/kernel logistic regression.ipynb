{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:\n",
    "=====\n",
    "\n",
    "multiclass logistic regression with the inner product replaced by a more general kernal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import this stuff\n",
    "import time\n",
    "import sys\n",
    "from pylab import *\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def factorize(observed_features,\n",
    "              labels,\n",
    "              observed_features_validation,\n",
    "              labels_validation,\n",
    "              max_iter=100,\n",
    "              verbose=False,\n",
    "              lambda_v=0,\n",
    "              kernel='linear',\n",
    "              epsilon=0.0001,\n",
    "              optimizer=tf.train.AdamOptimizer(),\n",
    "              seed=12345):\n",
    "\n",
    "    # Extract info about shapes etc from the training data\n",
    "    num_items = observed_features.shape[0]\n",
    "    num_features = observed_features.shape[1]\n",
    "    num_classes = labels.shape[1]\n",
    " \n",
    "    v = tf.Variable(tf.truncated_normal([num_classes, num_features], stddev=0.2, mean=0, seed=seed), name=\"hyperplane\")\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [1, num_features])\n",
    "    y = tf.placeholder(tf.float32, [1, num_classes])\n",
    "    x_norm = tf.nn.l2_normalize(x,dim=0)\n",
    "    \n",
    "    if kernel=='linear':\n",
    "        ip = tf.matmul(v, tf.transpose(x))\n",
    "    elif kernel=='gaussian':\n",
    "        ip = tf.reshape(tf.exp(-tf.reduce_sum(tf.square(tf.sub(v, x)), reduction_indices=1)/10), [num_classes, 1]) #dunno how to avoid this reshape\n",
    "    elif kernel=='quadratic':\n",
    "        ip = (1 + tf.matmul(v, tf.transpose(x)))*(1 + tf.matmul(v, tf.transpose(x)))\n",
    "    else:\n",
    "        raise Exception(\"unknown kernel: %s\" % kernel)\n",
    "\n",
    "\n",
    "    pred = tf.nn.softmax(tf.transpose(ip))\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred + 0.0000000001), reduction_indices=1) + # this was causing nans if pred == 0\n",
    "                          lambda_v*tf.nn.l2_loss(v)) # regularization for v\n",
    "    \n",
    "    norm = tf.nn.l2_loss(v)\n",
    "    optimize = optimizer.minimize(cost)\n",
    "\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        last_cost = 1000000\n",
    "        for iter in range(0, max_iter):\n",
    "            avg_cost = 0\n",
    "            \n",
    "            for i in range(num_items):\n",
    "                _, c, n = sess.run([optimize, cost, norm],\n",
    "                                   feed_dict={x:observed_features[i].reshape(1,num_features), y:labels[i].reshape(1,num_classes)})\n",
    "                avg_cost += c / num_items\n",
    "            if verbose:\n",
    "                print(\"epoch: %s, cost: %s, norm: %s\" % (iter+1, avg_cost, n))\n",
    "\n",
    "            # check for convergence\n",
    "            if abs(avg_cost-last_cost)/avg_cost < epsilon:\n",
    "                break\n",
    "                \n",
    "            last_cost = avg_cost\n",
    "            \n",
    "        if verbose:\n",
    "            print(\"optimization finished\")\n",
    "        # test prediction\n",
    "        predictions = []\n",
    "        total_costs = 0\n",
    "        for i in range(observed_features_validation.shape[0]):\n",
    "            p, c = sess.run([pred, cost], feed_dict={x:observed_features_validation[i].reshape(1, num_features), y:labels_validation[i].reshape(1, num_classes)})\n",
    "            predictions.append(p)\n",
    "            total_costs += c\n",
    "        return np.array([z[0,:] for z in predictions]), total_costs/observed_features_validation.shape[0], sess.run([norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this data for now\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "ng = datasets.fetch_20newsgroups (categories=categories, shuffle=True)\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "labels = encoder.fit_transform(ng.target.reshape(-1,1))\n",
    "\n",
    "tfidf = TfidfVectorizer(decode_error=False, min_df=5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ng.data, labels, test_size=.3)\n",
    "X_train = tfidf.fit_transform(X_train).todense()\n",
    "X_test = tfidf.transform(X_test).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1.31251387962, norm: 125.028\n",
      "epoch: 2, cost: 0.971380446533, norm: 153.94\n",
      "epoch: 3, cost: 0.842345043762, norm: 184.889\n",
      "epoch: 4, cost: 0.786399896987, norm: 206.277\n",
      "epoch: 5, cost: 0.759373590718, norm: 219.928\n",
      "epoch: 6, cost: 0.745056213477, norm: 228.823\n",
      "epoch: 7, cost: 0.736924188774, norm: 234.92\n",
      "epoch: 8, cost: 0.732061335828, norm: 239.303\n",
      "epoch: 9, cost: 0.729035899355, norm: 242.564\n",
      "epoch: 10, cost: 0.727092756566, norm: 245.05\n",
      "epoch: 11, cost: 0.72581127822, norm: 246.975\n",
      "epoch: 12, cost: 0.724946805935, norm: 248.485\n",
      "epoch: 13, cost: 0.724351918588, norm: 249.68\n",
      "epoch: 14, cost: 0.723935142358, norm: 250.633\n",
      "epoch: 15, cost: 0.723638324608, norm: 251.396\n",
      "epoch: 16, cost: 0.723423713291, norm: 252.012\n",
      "epoch: 17, cost: 0.723266330624, norm: 252.51\n",
      "epoch: 18, cost: 0.723149394257, norm: 252.916\n",
      "epoch: 19, cost: 0.723061424388, norm: 253.247\n",
      "epoch: 20, cost: 0.722994494412, norm: 253.518\n",
      "optimization finished\n",
      "cost: 0.786358038189, norm: [253.51556]\n",
      "class 0 AUC: 0.998884703083\n",
      "class 1 AUC: 0.997224614796\n",
      "class 2 AUC: 0.997328866353\n",
      "class 3 AUC: 0.988405892123\n",
      "overall AUC: 0.995209451747\n"
     ]
    }
   ],
   "source": [
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, verbose=True, lambda_v=0.001, max_iter=100, kernel='linear')\n",
    "print(\"cost: %s, norm: %s\") % (test_costs, norm)\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(\"class %s AUC: %s\") % (i, roc_auc_score(y_test[:,i], predictions[:,i]))\n",
    "print(\"overall AUC: %s\") % roc_auc_score(y_test, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1.15101790787, norm: 155.938\n",
      "epoch: 2, cost: 0.546666116485, norm: 160.701\n",
      "epoch: 3, cost: 0.352679439979, norm: 154.875\n",
      "epoch: 4, cost: 0.278717816093, norm: 146.835\n",
      "epoch: 5, cost: 0.243684271399, norm: 141.664\n",
      "epoch: 6, cost: 0.225549640942, norm: 139.085\n",
      "epoch: 7, cost: 0.215517245757, norm: 137.881\n",
      "epoch: 8, cost: 0.209615330943, norm: 137.325\n",
      "epoch: 9, cost: 0.205938151048, norm: 137.065\n",
      "epoch: 10, cost: 0.203515240131, norm: 136.944\n",
      "epoch: 11, cost: 0.201834365867, norm: 136.892\n",
      "epoch: 12, cost: 0.200617767035, norm: 136.878\n",
      "epoch: 13, cost: 0.199706033136, norm: 136.883\n",
      "epoch: 14, cost: 0.199002401862, norm: 136.899\n",
      "epoch: 15, cost: 0.198445891139, norm: 136.92\n",
      "epoch: 16, cost: 0.197996640126, norm: 136.944\n",
      "epoch: 17, cost: 0.197627772752, norm: 136.97\n",
      "epoch: 18, cost: 0.197320593449, norm: 136.995\n",
      "epoch: 19, cost: 0.197061796725, norm: 137.021\n",
      "epoch: 20, cost: 0.19684158276, norm: 137.046\n",
      "epoch: 21, cost: 0.196652625863, norm: 137.071\n",
      "epoch: 22, cost: 0.196489363475, norm: 137.096\n",
      "epoch: 23, cost: 0.196347411776, norm: 137.12\n",
      "epoch: 24, cost: 0.196223380494, norm: 137.144\n",
      "epoch: 25, cost: 0.196114483412, norm: 137.168\n",
      "epoch: 26, cost: 0.19601851966, norm: 137.191\n",
      "epoch: 27, cost: 0.195933649204, norm: 137.214\n",
      "epoch: 28, cost: 0.195858365848, norm: 137.236\n",
      "epoch: 29, cost: 0.195791387413, norm: 137.258\n",
      "epoch: 30, cost: 0.19573169371, norm: 137.279\n",
      "epoch: 31, cost: 0.195678341655, norm: 137.299\n",
      "epoch: 32, cost: 0.195630591549, norm: 137.32\n",
      "epoch: 33, cost: 0.195587766442, norm: 137.34\n",
      "epoch: 34, cost: 0.195549308574, norm: 137.359\n",
      "epoch: 35, cost: 0.195514726386, norm: 137.378\n",
      "epoch: 36, cost: 0.195483577099, norm: 137.396\n",
      "epoch: 37, cost: 0.195455529112, norm: 137.414\n",
      "epoch: 38, cost: 0.195430198487, norm: 137.431\n",
      "epoch: 39, cost: 0.195407347288, norm: 137.448\n",
      "epoch: 40, cost: 0.195386679009, norm: 137.464\n",
      "epoch: 41, cost: 0.195367999314, norm: 137.48\n",
      "optimization finished\n",
      "cost: 0.293843073699, norm: [137.47137]\n",
      "class 0 AUC: 0.999851293744\n",
      "class 1 AUC: 0.998576418796\n",
      "class 2 AUC: 0.9982909272\n",
      "class 3 AUC: 0.997967479675\n",
      "overall AUC: 0.998624828154\n"
     ]
    }
   ],
   "source": [
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, verbose=True, lambda_v=0.001, max_iter=100, kernel='quadratic')\n",
    "print(\"cost: %s, norm: %s\") % (test_costs, norm)\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(\"class %s AUC: %s\") % (i, roc_auc_score(y_test[:,i], predictions[:,i]))\n",
    "print(\"overall AUC: %s\") % roc_auc_score(y_test, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, cost: 1.42080037189, norm: 11.8809\n",
      "epoch: 2, cost: 1.37131457596, norm: 7.68381\n",
      "epoch: 3, cost: 1.36449434645, norm: 7.293\n",
      "epoch: 4, cost: 1.3634120469, norm: 7.34623\n",
      "epoch: 5, cost: 1.36320340618, norm: 7.42745\n",
      "epoch: 6, cost: 1.36317231385, norm: 7.49394\n",
      "optimization finished\n",
      "cost: 1.36266632558, norm: [7.4852238]\n",
      "class 0 AUC: 0.981919797759\n",
      "class 1 AUC: 0.975224901904\n",
      "class 2 AUC: 0.986542466498\n",
      "class 3 AUC: 0.901717807501\n",
      "overall AUC: 0.959517801272\n"
     ]
    }
   ],
   "source": [
    "predictions, test_costs, norm = factorize(X_train, y_train, X_test, y_test, verbose=True, lambda_v=0.001, max_iter=100, kernel='gaussian')\n",
    "print(\"cost: %s, norm: %s\") % (test_costs, norm)\n",
    "for i in range(y_train.shape[1]):\n",
    "    print(\"class %s AUC: %s\") % (i, roc_auc_score(y_test[:,i], predictions[:,i]))\n",
    "print(\"overall AUC: %s\") % roc_auc_score(y_test, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
